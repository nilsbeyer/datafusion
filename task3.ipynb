{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import numpy as np\n",
    "from celluloid import Camera\n",
    "from pyquaternion import Quaternion\n",
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "\n",
    "mpl.rc('figure', max_open_warning = 0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc = NuScenes(version='v1.0-mini', dataroot='./data/nusc', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample = nusc.sample[0]\n",
    "sample_data = nusc.get('sample_data', my_sample['data']['LIDAR_TOP'])\n",
    "pcl_path = sample_data['filename']\n",
    "nusc.render_sample_data(sample_data['token'])\n",
    "pc =LidarPointCloud.from_file(os.path.join('./data/nusc',pcl_path))\n",
    "color = pc.points[3] / np.max(pc.points[3])\n",
    "plt.scatter(pc.points[0], pc.points[1], c=color, alpha=0.2, s=0.3)\n",
    "\n",
    "# plt.savefig(\"sample.jpg\")\n",
    "\n",
    "# image = cv2.imread(\"sample.jpg\")\n",
    "# # convert the image into RGB format\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# # reshape the image to a 2D array of pixels and 3 color values (RGB)\n",
    "# pixel_values = image.reshape((-1, 3))\n",
    "\n",
    "# # pixel_values = np.delete(pc.points, 2, 0).T\n",
    "# # print(test.shape)\n",
    "# # convert to float\n",
    "# pixel_values = np.float32(pixel_values)\n",
    "\n",
    "# criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 1)\n",
    "# # number of clusters (K)\n",
    "# k = 12\n",
    "# _, labels, (centers) = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "# # convert back to 8 bit values\n",
    "# centers = np.uint8(centers)\n",
    "\n",
    "# # flatten the labels array\n",
    "# labels = labels.flatten()\n",
    "# # convert all pixels to the color of the centroids\n",
    "# segmented_image = centers[labels.flatten()]\n",
    "# # reshape back to the original image dimension\n",
    "# segmented_image = segmented_image.reshape(image.shape)\n",
    "# # show the image\n",
    "# plt.figure(figsize = (16,8))\n",
    "# plt.imshow(segmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pc.points[0:2].T\n",
    "kmeans = KMeans(n_clusters=69)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=0.2, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=40, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lidar_pointcloud(sample_token):\n",
    "    sample_record = nusc.get('sample', sample_token)\n",
    "    pointsensor_channel = 'LIDAR_TOP'\n",
    "    pointsensor_token = sample_record['data'][pointsensor_channel]\n",
    "    pointsensor = nusc.get('sample_data', pointsensor_token)\n",
    "    pcl_path = nusc.get('sample_data', pointsensor_token)['filename']\n",
    "    pc =LidarPointCloud.from_file(os.path.join('./data/nusc',pcl_path))\n",
    "    return pointsensor_token, pc.points\n",
    "def rotate_pointcloud(cloud, quaternion):\n",
    "    def rot(an_array):\n",
    "        return quaternion.rotate(an_array)\n",
    "    return np.array(list(map(rot, cloud)))\n",
    "def visualize_absolute(ax, absolute_points, ego_pose, van_translation, relativeboxvectors):\n",
    "    axes = plt.gca()\n",
    "    return ax.scatter(absolute_points[0], absolute_points[1], c='red', s=0.3), ax.scatter(ego_pose['translation'][0], ego_pose['translation'][1]),ax.plot([van_translation[0] + relativeboxvector[0] for relativeboxvector in relativeboxvectors], [van_translation[1] + relativeboxvector[1] for relativeboxvector in relativeboxvectors])[0]\n",
    "def create_bounding_box(annotation_token):\n",
    "    '''\n",
    "    creates bounding box used for plotting\n",
    "    '''\n",
    "    van_annotation = nusc.get('sample_annotation', annotation_token)\n",
    "    relativeboxvectors = [(van_annotation['size'][1]/2,van_annotation['size'][0]/2, 0),\n",
    "                         (-van_annotation['size'][1]/2,van_annotation['size'][0]/2, 0),\n",
    "                         (-van_annotation['size'][1]/2,-van_annotation['size'][0]/2, 0),\n",
    "                         (van_annotation['size'][1]/2,-van_annotation['size'][0]/2, 0),\n",
    "                         (van_annotation['size'][1]/2,van_annotation['size'][0]/2, 0)]\n",
    "    return np.array(relativeboxvectors), van_annotation['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_tokens = my_sample['anns']\n",
    "X_data = np.array([])\n",
    "y_data = np.array([])\n",
    "for i,current_van_annotation_token in enumerate(ann_tokens):\n",
    "    van_annotation = nusc.get('sample_annotation', current_van_annotation_token)\n",
    "    pointsensor_token, points = get_lidar_pointcloud(my_sample['token'])\n",
    "    calibrated_sensor_token = nusc.get('sample_data', pointsensor_token)['calibrated_sensor_token']\n",
    "    calibrated_sensor = nusc.get('calibrated_sensor', calibrated_sensor_token)\n",
    "    ego_pose = nusc.get('ego_pose', nusc.get('sample_data', pointsensor_token)['ego_pose_token'])\n",
    "\n",
    "    ### Rotate points around sensor and ego rotation\n",
    "    pointsensor_quaternion = Quaternion(calibrated_sensor['rotation'])\n",
    "    ego_quaternion = Quaternion(ego_pose['rotation'])\n",
    "\n",
    "    sensor_rotated_points = np.dot(pointsensor_quaternion.rotation_matrix, points[:3,:])\n",
    "    rotated_points = np.dot(ego_quaternion.rotation_matrix, sensor_rotated_points)\n",
    "\n",
    "    # use broadcasting to add translation to x and y dimension\n",
    "    absolute_points = rotated_points+np.array(ego_pose['translation'][:3]).reshape(-1,1)\n",
    "\n",
    "    ### rotate points by inverse bounding box rotation (translation to its reference frame unnecessary for only filtering)\n",
    "    reverse_van_quaternion = Quaternion(w=-1*van_annotation['rotation'][0], x=van_annotation['rotation'][1], y=van_annotation['rotation'][2], z=van_annotation['rotation'][3])\n",
    "    points_bounding_rotation = np.dot(reverse_van_quaternion.rotation_matrix, absolute_points)\n",
    "\n",
    "    #new stuff\n",
    "    relativeboxvectors, position = create_bounding_box(current_van_annotation_token)\n",
    "\n",
    "    rotated_box = relativeboxvectors+reverse_van_quaternion.rotate(position)\n",
    "\n",
    "    x_max = rotated_box[:,0].max()\n",
    "    x_min = rotated_box[:,0].min()\n",
    "\n",
    "    y_max = rotated_box[:,1].max()\n",
    "    y_min = rotated_box[:,1].min()\n",
    "\n",
    "    ## Remove points that are outside the bounding box\n",
    "    mask = np.ones(points_bounding_rotation.shape[1], dtype=bool)\n",
    "\n",
    "    mask = np.logical_and(mask, points_bounding_rotation[0] > x_min)\n",
    "    mask = np.logical_and(mask, points_bounding_rotation[0] < x_max)\n",
    "\n",
    "    mask = np.logical_and(mask, points_bounding_rotation[1] > y_min)\n",
    "    mask = np.logical_and(mask, points_bounding_rotation[1] < y_max)\n",
    "\n",
    "    points_filtered_rotated = points_bounding_rotation[:,mask]\n",
    "\n",
    "    ### rotate points back\n",
    "    van_quaternion = Quaternion(van_annotation['rotation'])\n",
    "    points_filtered_absolute = np.dot(van_quaternion.rotation_matrix, points_filtered_rotated)\n",
    "\n",
    "    for px in points_filtered_absolute[0]:\n",
    "        X_data = np.append(X_data, px)\n",
    "    for py in points_filtered_absolute[1]:\n",
    "        y_data = np.append(y_data, py)\n",
    "X_data = np.reshape(X_data, (X_data.shape[0], 1))\n",
    "y_data = np.reshape(y_data, (y_data.shape[0], 1))\n",
    "data = np.append(X_data, y_data, axis=1)\n",
    "# Implement K-means\n",
    "kmeans = KMeans(n_clusters=69)\n",
    "kmeans.fit(data)\n",
    "y_kmeans = kmeans.predict(data)\n",
    "plt.scatter(data[:, 0], data[:, 1], c=y_kmeans, s=1, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=40, alpha=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
