{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/nils/data_fusion/nuscenes-devkit/python-sdk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.nuscenes import NuScenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc = NuScenes(version='v1.0-mini', dataroot='nuscenes', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.utils.data_classes import LidarPointCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.utils.geometry_utils import view_points\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquaternion import Quaternion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scene\n",
    "20 second snippet of a car's journey.\n",
    "A scene is a 20s long sequence of consecutive frames extracted from a log. Multiple scenes can come from the same log. Note that object identities (instance tokens) are not preserved across scenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance\n",
    "Enumeration of all object instance we observed.\n",
    "An object instance, e.g. particular vehicle. This table is an enumeration of all object instances we observed. Note that instances are not tracked across scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample\n",
    "An annotated snapshot of a scene at a particular timestamp.\n",
    "A sample is data collected at (approximately) the same timestamp as part of a single LIDAR sweep.\n",
    "We define sample as an annotated keyframe of a scene at a given timestamp. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A keyframe is a frame where the time-stamps of data from all the sensors should be very close to the time-stamp of the sample it points to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Annotation\n",
    "An annotated instance of an object within our interest.\n",
    "A bounding box defining the position of an object seen in a sample. All location data is given with respect to the global coordinate system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample = nusc.sample[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scene_token =my_sample['scene_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scene = nusc.get('scene', my_scene_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick first frame of scene\n",
    "first_sample_token = my_scene['first_sample_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample_from_scene = nusc.get('sample', first_sample_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the van in front by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_annotation_tokens = []\n",
    "for i in range(len(first_sample_from_scene['anns'])):\n",
    "    first_annotation_token_of_first_sample = first_sample_from_scene['anns'][i]\n",
    "    if nusc.get('sample_annotation', first_annotation_token_of_first_sample)['category_name'] == 'vehicle.car':\n",
    "        vehicle_annotation_tokens.append(first_annotation_token_of_first_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation of first frame\n",
    "van_annotation_token = vehicle_annotation_tokens[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance can also be plotteded directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_instance_token = nusc.get('sample_annotation', van_annotation_token)['instance_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc.get('instance', my_instance_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc.render_instance(my_instance_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the van in the first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc.render_annotation(van_annotation_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the van in the next frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_annotation_token = nusc.get('sample_annotation', van_annotation_token)['next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nusc.render_annotation(next_annotation_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_annotation_token = nusc.get('sample_annotation', next_annotation_token)['next']\n",
    "\n",
    "#nusc.render_annotation(next_annotation_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_token_of_last_sample = nusc.get('instance', my_instance_token)['last_annotation_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nusc.render_annotation(annotation_token_of_last_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b + 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_front(sample_token):\n",
    "    # visualizing the frame with front camera\n",
    "    sample_record = nusc.get('sample', sample_token)\n",
    "    camera_channel = 'CAM_FRONT'\n",
    "    camera_token = sample_record['data'][camera_channel]\n",
    "    cam = nusc.get('sample_data', camera_token)\n",
    "    im = Image.open(os.path.join(nusc.dataroot, cam['filename']))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lidar_pointcloud(sample_token):\n",
    "    sample_record = nusc.get('sample', sample_token)\n",
    "    pointsensor_channel = 'LIDAR_TOP'\n",
    "    pointsensor_token = sample_record['data'][pointsensor_channel]\n",
    "    pointsensor = nusc.get('sample_data', pointsensor_token)\n",
    "    pcl_path = nusc.get('sample_data', pointsensor_token)['filename']\n",
    "    pc =LidarPointCloud.from_file(os.path.join('nuscenes',pcl_path))\n",
    "    return pointsensor_token, pc.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_pointcloud(cloud, quaternion):\n",
    "    def rot(an_array):\n",
    "        return quaternion.rotate(an_array)\n",
    "    swapped_axis = np.array(list(map(rot, np.swapaxes(cloud,0,1))))\n",
    "    return np.swapaxes(swapped_axis,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_absolute(camera, ax, absolute_points, ego_pose, van_translation, relativeboxvectors):\n",
    "    #plt.axis('equal')\n",
    "    #axes = plt.gca()\n",
    "    #axes.set_xlim([xmin,xmax])\n",
    "    #axes.set_ylim([ymin,ymax])\n",
    "    #ax.set(xlim=(300, 500), ylim=(1100, 1300))\n",
    "    ax.scatter(absolute_points[0], absolute_points[1], c='red', s=0.5)\n",
    "    ax.scatter(ego_pose['translation'][0], ego_pose['translation'][1])\n",
    "    ax.plot([van_translation[0] + relativeboxvector[0] for relativeboxvector in relativeboxvectors], [van_translation[1] + relativeboxvector[1] for relativeboxvector in relativeboxvectors])\n",
    "    camera.snap()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first annotation token\n",
    "current_van_annotation_token = van_annotation_token\n",
    "annotation_tokens = []\n",
    "while current_van_annotation_token != '':\n",
    "    annotation_tokens.append(current_van_annotation_token)\n",
    "    #next annotaton token\n",
    "    current_van_annotation_token = nusc.get('sample_annotation', current_van_annotation_token)['next']\n",
    "#annotation_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import celluloid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ims = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)  # fig and axes created once\n",
    "camera = celluloid.Camera(plt.figure())\n",
    "\n",
    "for i,current_van_annotation_token in enumerate(annotation_tokens):\n",
    "    \n",
    "    van_annotation = nusc.get('sample_annotation', current_van_annotation_token)\n",
    "    #van_annotation['rotation']#w, x, y, z\n",
    "    #van_annotation['translation']# center_x, center_y, center_z\n",
    "\n",
    "    #get respective frame token\n",
    "    sample_token = van_annotation['sample_token']\n",
    "\n",
    "    #im = visualize_sample_front(sample_token)\n",
    "    #im\n",
    "    pointsensor_token, points = get_lidar_pointcloud(sample_token)\n",
    "    calibrated_sensor_token = nusc.get('sample_data', pointsensor_token)['calibrated_sensor_token']\n",
    "    calibrated_sensor = nusc.get('calibrated_sensor', calibrated_sensor_token)\n",
    "    ego_pose = nusc.get('ego_pose', nusc.get('sample_data', pointsensor_token)['ego_pose_token'])\n",
    "\n",
    "    \n",
    "    #rotation\n",
    "    pointsensor_quaternion = Quaternion(calibrated_sensor['rotation'])\n",
    "    #pointsensor_quaternion = Quaternion(w=1*calibrated_sensor['rotation'][0], x=calibrated_sensor['rotation'][1], y=calibrated_sensor['rotation'][2], z=calibrated_sensor['rotation'][3])\n",
    "    van_quaternion = Quaternion(van_annotation['rotation'])\n",
    "    #van_quaternion = Quaternion(w=1*van_annotation['rotation'][0], x=van_annotation['rotation'][1], y=van_annotation['rotation'][2], z=van_annotation['rotation'][3])\n",
    "    reverse_van_quaternion = Quaternion(w=-1*van_annotation['rotation'][0], x=van_annotation['rotation'][1], y=van_annotation['rotation'][2], z=van_annotation['rotation'][3])\n",
    "    ego_quaternion = Quaternion(w=1*ego_pose['rotation'][0], x=ego_pose['rotation'][1], y=ego_pose['rotation'][2], z=ego_pose['rotation'][3])\n",
    "    #rotated_points = rotate_pointcloud(points[:3,:], ego_quaternion)\n",
    "    sensor_rotated_points = np.dot(pointsensor_quaternion.rotation_matrix, points[:3,:])\n",
    "    rotated_points = np.dot(ego_quaternion.rotation_matrix, sensor_rotated_points)\n",
    "\n",
    "    # use broadcasting to add translation to x and y dimension\n",
    "    absolute_points = rotated_points+np.array(ego_pose['translation'][:3]).reshape(-1,1)\n",
    "\n",
    "    ## displaying points and bounding box in global coordinates\n",
    "\n",
    "    ### rotated bounding box\n",
    "\n",
    "    #van_annotation['size']#bounding box size: width, length, height\n",
    "    #van_quaternion.rotate(van_annotation['size'])\n",
    "    relativeboxvectors = [(van_annotation['size'][1]/2,van_annotation['size'][0]/2, 0),\n",
    "                         (-van_annotation['size'][1]/2,van_annotation['size'][0]/2, 0),\n",
    "                         (-van_annotation['size'][1]/2,-van_annotation['size'][0]/2, 0),\n",
    "                         (van_annotation['size'][1]/2,-van_annotation['size'][0]/2, 0),\n",
    "                         (van_annotation['size'][1]/2,van_annotation['size'][0]/2, 0)]\n",
    "\n",
    "    for j in range(5):\n",
    "        relativeboxvectors[j] = van_quaternion.rotate(relativeboxvectors[j])\n",
    "\n",
    "    plotlist = visualize_absolute(camera, ax, absolute_points, ego_pose, van_annotation['translation'], relativeboxvectors)\n",
    "    \n",
    "    ims.append(plotlist)      # append the new list of plots\n",
    "    #ax.scatter(absolute_points[0], absolute_points[1], c='red', s=0.2)\n",
    "    #ax.scatter(ego_pose['translation'][0], ego_pose['translation'][1]) \n",
    "    van_translation = van_annotation['translation']\n",
    "    #ax.plot([van_translation[0] + relativeboxvector[0] for relativeboxvector in relativeboxvectors], [van_translation[1] + relativeboxvector[1] for relativeboxvector in relativeboxvectors])\n",
    "    #print(f'progress: {i/len(annotation_tokens)}')\n",
    "ax.axis('equal')\n",
    "#ani = mpl.animation.ArtistAnimation(fig, ims, repeat=False)\n",
    "anim = camera.animate(blit=True)\n",
    "anim.save(\"test.gif\", writer='pillow')\n",
    "\n",
    "#ani.save('im.gif', writer='pillow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_quaternion.rotation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_quaternion.rotation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_sensor = nusc.get('sample_data', pointsensor_token)['calibrated_sensor_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering out points outside bounding box\n",
    "\n",
    "### translating points to car reference frame\n",
    "\n",
    "#ego_quaternion = Quaternion(w=1*ego_pose['rotation'][0], x=ego_pose['rotation'][1], y=ego_pose['rotation'][2], z=ego_pose['rotation'][3])\n",
    "#ego_quaternion.rotate(np.array((0,1,0)))\n",
    "\n",
    "# use broadcasting to add translation to x and y dimension\n",
    "points_in_van_reference = absolute_points-np.array(van_annotation['translation']).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## displaying points in van reference\n",
    "plt.scatter(points_in_van_reference[0], points_in_van_reference[1], c='red', s=0.5)\n",
    "plt.plot((0,ego_pose['translation'][0]-van_annotation['translation'][0]),(0,ego_pose['translation'][1]-van_annotation['translation'][1]))\n",
    "plt.plot([relativeboxvector[0] for relativeboxvector in relativeboxvectors]\n",
    "          , [relativeboxvector[1] for relativeboxvector in relativeboxvectors])\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_in_van_reference_rotated = rotate_pointcloud(points_in_van_reference, reverse_van_quaternion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rotated_reference_vector =reverse_van_quaternion.rotate((ego_x_translation[0]-van_x_translation[0],ego_y_translation[9]-van_y_translation[0],0))\n",
    "\n",
    "## displaying points in van reference including orientation\n",
    "plt.scatter(points_in_van_reference_rotated[0], points_in_van_reference_rotated[1], c='red', s=0.5)\n",
    "plt.plot((0,rotated_reference_vector[0]),(0,rotated_reference_vector[1]))\n",
    "plt.plot([relativeboxvector[0] for relativeboxvector in relativeboxvectors]\n",
    "          , [relativeboxvector[1] for relativeboxvector in relativeboxvectors])\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove points that are outside the bounding box\n",
    "depths = pc.points[2, :]\n",
    "depths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(depths.shape[0], dtype=bool)\n",
    "#rotated[0], rotated[1]\n",
    "\n",
    "\n",
    "mask = np.logical_and(mask, rotated[0] > -(van_annotation['size'][0])/2)\n",
    "mask = np.logical_and(mask, rotated[0] < (van_annotation['size'][0])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.logical_and(mask, rotated[1] > -(van_annotation['size'][1])/2)\n",
    "mask = np.logical_and(mask, rotated[1] < (van_annotation['size'][1])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "#mask = np.logical_and(mask, points[1, :] > 1)\n",
    "#mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "\n",
    "#points = points[:, mask]\n",
    "#absolute_points_x = absolute_points_x[:, mask]\n",
    "#coloring = depths\n",
    "#coloring = coloring[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_rotated_x = rotated[0][mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_rotated_y = rotated[1][mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## displaying points in van reference\n",
    "plt.scatter(masked_rotated_x, masked_rotated_y, c='red', s=0.5)\n",
    "plt.plot((0,rotated_reference_vector[0]),(0,rotated_reference_vector[1]))\n",
    "plt.plot([relativeboxvector[0] for relativeboxvector in relativeboxvectors]\n",
    "          , [relativeboxvector[1] for relativeboxvector in relativeboxvectors])\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## builtin render sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_data = nusc.get('sample_data', my_sample['data']['LIDAR_TOP'])\n",
    "cam_front_data = nusc.get('sample_data', my_sample['data']['CAM_FRONT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc.render_sample_data(cam_front_data['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc.render_sample_data(lidar_data['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc.render_pointcloud_in_image(my_sample['token'], pointsensor_channel='LIDAR_TOP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc.render_sample_data(my_sample['data']['CAM_FRONT'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
