{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from pyquaternion import Quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/nils/data_fusion/nuscenes-devkit/python-sdk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='nuscenes', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "\n",
    "from nuscenes.utils.geometry_utils import view_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample = nusc.sample[10]\n",
    "\n",
    "my_scene_token =my_sample['scene_token']\n",
    "\n",
    "my_scene = nusc.get('scene', my_scene_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample_token = my_scene['first_sample_token']\n",
    "\n",
    "first_sample_from_scene = nusc.get('sample', first_sample_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the van in front by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_annotation_tokens = []\n",
    "for i in range(len(first_sample_from_scene['anns'])):\n",
    "    first_annotation_token_of_first_sample = first_sample_from_scene['anns'][i]\n",
    "    if nusc.get('sample_annotation', first_annotation_token_of_first_sample)['category_name'] == 'vehicle.car':\n",
    "        vehicle_annotation_tokens.append(first_annotation_token_of_first_sample)\n",
    "\n",
    "# annotation of first frame\n",
    "van_annotation_token = vehicle_annotation_tokens[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lidar_pointcloud(sample_token):\n",
    "    sample_record = nusc.get('sample', sample_token)\n",
    "    pointsensor_channel = 'LIDAR_TOP'\n",
    "    pointsensor_token = sample_record['data'][pointsensor_channel]\n",
    "    pointsensor = nusc.get('sample_data', pointsensor_token)\n",
    "    pcl_path = nusc.get('sample_data', pointsensor_token)['filename']\n",
    "    pc =LidarPointCloud.from_file(os.path.join('nuscenes',pcl_path))\n",
    "    return pointsensor_token, pc.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_pointcloud(cloud, quaternion):\n",
    "    def rot(an_array):\n",
    "        return quaternion.rotate(an_array)\n",
    "    return np.array(list(map(rot, cloud)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_absolute(ax, absolute_points, ego_pose, van_translation, relativeboxvectors):\n",
    "    #plt.axis('equal')\n",
    "    axes = plt.gca()\n",
    "    #axes.set_xlim([xmin,xmax])\n",
    "    #axes.set_ylim([ymin,ymax])\n",
    "    #ax.set(xlim=(300, 500), ylim=(1100, 1300))\n",
    "    return ax.scatter(absolute_points[0], absolute_points[1], c='red', s=0.3), ax.scatter(ego_pose['translation'][0], ego_pose['translation'][1]),ax.plot([van_translation[0] + relativeboxvector[0] for relativeboxvector in relativeboxvectors], [van_translation[1] + relativeboxvector[1] for relativeboxvector in relativeboxvectors])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bounding_box(annotation_token):\n",
    "    '''\n",
    "    creates bounding box used for plotting\n",
    "    '''\n",
    "    van_annotation = nusc.get('sample_annotation', annotation_token)\n",
    "    relativeboxvectors = [(van_annotation['size'][1]/2,van_annotation['size'][0]/2, 0),\n",
    "                         (-van_annotation['size'][1]/2,van_annotation['size'][0]/2, 0),\n",
    "                         (-van_annotation['size'][1]/2,-van_annotation['size'][0]/2, 0),\n",
    "                         (van_annotation['size'][1]/2,-van_annotation['size'][0]/2, 0),\n",
    "                         (van_annotation['size'][1]/2,van_annotation['size'][0]/2, 0)]\n",
    "    return np.array(relativeboxvectors), van_annotation['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of annotation tokens\n",
    "current_van_annotation_token = van_annotation_token\n",
    "annotation_tokens = []\n",
    "while current_van_annotation_token != '':\n",
    "    annotation_tokens.append(current_van_annotation_token)\n",
    "    #next annotaton token\n",
    "    current_van_annotation_token = nusc.get('sample_annotation', current_van_annotation_token)['next']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as last time. Now we just rotate back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)  # fig and axes created once\n",
    "frontimages = []\n",
    "raw_measurements = []\n",
    "timestamps = []\n",
    "annotated_positions = []\n",
    "# Display points and bounding box in global coordinates over whole scene\n",
    "for i,current_van_annotation_token in enumerate(annotation_tokens):\n",
    "\n",
    "    # filter out points outside bounding box\n",
    "\n",
    "\n",
    "    ## Grab the data\n",
    "\n",
    "    van_annotation = nusc.get('sample_annotation', current_van_annotation_token)\n",
    "\n",
    "    #get respective frame token\n",
    "    sample_token = van_annotation['sample_token']\n",
    "\n",
    "    timestamps.append(nusc.get('sample', sample_token)['timestamp'])\n",
    "\n",
    "\n",
    "    pointsensor_token, points = get_lidar_pointcloud(sample_token)\n",
    "    calibrated_sensor_token = nusc.get('sample_data', pointsensor_token)['calibrated_sensor_token']\n",
    "    calibrated_sensor = nusc.get('calibrated_sensor', calibrated_sensor_token)\n",
    "    ego_pose = nusc.get('ego_pose', nusc.get('sample_data', pointsensor_token)['ego_pose_token'])\n",
    "\n",
    "    ## translate points to global reference frame\n",
    "\n",
    "    ### Rotate points around sensor and ego rotation\n",
    "\n",
    "    pointsensor_quaternion = Quaternion(calibrated_sensor['rotation'])\n",
    "    ego_quaternion = Quaternion(ego_pose['rotation'])\n",
    "\n",
    "    sensor_rotated_points = np.dot(pointsensor_quaternion.rotation_matrix, points[:3,:])\n",
    "    rotated_points = np.dot(ego_quaternion.rotation_matrix, sensor_rotated_points)\n",
    "\n",
    "    # use broadcasting to add translation to x and y dimension\n",
    "    absolute_points = rotated_points+np.array(ego_pose['translation'][:3]).reshape(-1,1)\n",
    "\n",
    "\n",
    "    ### rotate points by inverse bounding box rotation (translation to its reference frame unnecessary for only filtering)\n",
    "    reverse_van_quaternion = Quaternion(w=-1*van_annotation['rotation'][0], x=van_annotation['rotation'][1], y=van_annotation['rotation'][2], z=van_annotation['rotation'][3])\n",
    "    points_bounding_rotation = np.dot(reverse_van_quaternion.rotation_matrix, absolute_points)\n",
    "\n",
    "    #new stuff\n",
    "    relativeboxvectors, position = create_bounding_box(current_van_annotation_token)\n",
    "\n",
    "    annotated_positions.append(position)\n",
    "    rotated_box = relativeboxvectors+reverse_van_quaternion.rotate(position)\n",
    "\n",
    "    x_max = rotated_box[:,0].max()\n",
    "    x_min = rotated_box[:,0].min()\n",
    "\n",
    "    y_max = rotated_box[:,1].max()\n",
    "    y_min = rotated_box[:,1].min()\n",
    "\n",
    "    ## Remove points that are outside the bounding box\n",
    "    mask = np.ones(points_bounding_rotation.shape[1], dtype=bool)\n",
    "\n",
    "    mask = np.logical_and(mask, points_bounding_rotation[0] > x_min)\n",
    "    mask = np.logical_and(mask, points_bounding_rotation[0] < x_max)\n",
    "\n",
    "    mask = np.logical_and(mask, points_bounding_rotation[1] > y_min)\n",
    "    mask = np.logical_and(mask, points_bounding_rotation[1] < y_max)\n",
    "\n",
    "    points_filtered_rotated = points_bounding_rotation[:,mask]\n",
    "\n",
    "    ### rotate points back\n",
    "\n",
    "    van_quaternion = Quaternion(van_annotation['rotation'])\n",
    "    points_filtered_absolute = np.dot(van_quaternion.rotation_matrix, points_filtered_rotated)\n",
    "    # append list of plots to movie\n",
    "    rotated_relativeboxvectors = rotate_pointcloud(relativeboxvectors, van_quaternion)\n",
    "    plotlist = visualize_absolute(ax, points_filtered_absolute, ego_pose, van_annotation['translation'], rotated_relativeboxvectors)\n",
    "    raw_measurements.append(points_filtered_absolute)\n",
    "    ims.append(plotlist)     \n",
    "\n",
    "ax.axis('equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = mpl.animation.ArtistAnimation(fig, ims, repeat=False)\n",
    "ani.save('filtered_im.gif', writer='pillow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_step(state_vector, covariance_matrix, sigma_v, t):\n",
    "    F = np.array(((1,0,t,0),(0,1,0,t),(0,0,1,0),(0,0,0,1)))\n",
    "    transition_error = np.array((0.5*t**2, 0.5*t**2, t, t))\n",
    "    Q = sigma_v**2*np.dot(transition_error.reshape(-1,1),transition_error.reshape(-1,1).T)\n",
    "    \n",
    "    state_vector_next = np.dot(F, state_vector)\n",
    "    C_next = np.dot(np.dot(F, covariance_matrix), F.T) + Q\n",
    "    return state_vector_next, C_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measuring_step(state_vector, C, measurement, R):\n",
    "    H = np.array(((1, 0, 0, 0), (0,1,0,0)))\n",
    "    S = np.dot(np.dot(H,C), H.T) + R\n",
    "    K = np.dot(np.dot(C, H.T), np.linalg.inv(S))\n",
    "    C_next = C - np.dot(np.dot(K, S),K.T)\n",
    "    state_vector_next = state_vector + np.dot(K,np.array(measurement) - np.dot(H, state_vector))\n",
    "    return state_vector_next, C_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = []\n",
    "t_last = timestamps[0]\n",
    "for r, t in zip(raw_measurements, timestamps):\n",
    "    x_mean = r[0].mean()\n",
    "    y_mean = r[1].mean()\n",
    "    measurements.append((x_mean, y_mean, (t-t_last)/1000000))\n",
    "    t_last = t\n",
    "# throw first 11 away because some are empty\n",
    "cleaned_measurements = measurements[11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_filter(C,sigma_v,R):\n",
    "    initial_position = np.array(cleaned_measurements[1][:2])\n",
    "    initial_velocity = (np.array(cleaned_measurements[1][:2]) - np.array(cleaned_measurements[0][:2]))/cleaned_measurements[0][2]\n",
    "    state_vector = np.append(initial_position,initial_velocity)\n",
    "\n",
    "    estimated_positions = []\n",
    "    for i,m in enumerate(cleaned_measurements):\n",
    "        state_vector, C = prediction_step(state_vector, C, sigma_v, t = m[2])\n",
    "        estimated_positions.append(state_vector)\n",
    "        state_vector, C = measuring_step(state_vector, C, m[:2], R)\n",
    "    return estimated_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_distance(v1, v2):\n",
    "    assert len(v1) == len(v2), \"vectors have different lengths\"\n",
    "    summe = 0\n",
    "    for i in range(len(v1)):\n",
    "        summe += (v1[i][0]-v2[i][0])**2\n",
    "        summe += (v1[i][1]-v2[i][1])**2\n",
    "    return summe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.3*np.eye(4)\n",
    "sigma_v= 0.8\n",
    "r = 0.01\n",
    "R = np.array(((r,0),(0,r)))\n",
    "\n",
    "estimated_positions = run_filter(C,sigma_v,R)\n",
    "\n",
    "print(f'error:{squared_distance(estimated_positions, annotated_positions[11:])}')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot([pos[0] for pos in estimated_positions], [pos[1] for pos in estimated_positions])\n",
    "plt.plot([pos[0] for pos in annotated_positions[11:]], [pos[1] for pos in annotated_positions[11:]])\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
