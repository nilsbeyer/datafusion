{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/nils/data_fusion/nuscenes-devkit/python-sdk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='nuscenes', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "\n",
    "from nuscenes.utils.geometry_utils import view_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_pointcloud(cloud, quaternion):\n",
    "    def rot(an_array):\n",
    "        return quaternion.rotate(an_array)\n",
    "    return np.array(list(map(rot, cloud)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bounding_box(annotation_token):\n",
    "    '''\n",
    "    creates bounding box used for plotting\n",
    "    '''\n",
    "    van_annotation = nusc.get('sample_annotation', annotation_token)\n",
    "    relativeboxvectors = [(van_annotation['size'][1]/2,van_annotation['size'][0]/2, 0),\n",
    "                         (-van_annotation['size'][1]/2,van_annotation['size'][0]/2, 0),\n",
    "                         (-van_annotation['size'][1]/2,-van_annotation['size'][0]/2, 0),\n",
    "                         (van_annotation['size'][1]/2,-van_annotation['size'][0]/2, 0),\n",
    "                         (van_annotation['size'][1]/2,van_annotation['size'][0]/2, 0)]\n",
    "    return np.array(relativeboxvectors), van_annotation['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample = nusc.sample[10]\n",
    "my_scene_token =my_sample['scene_token']\n",
    "my_scene = nusc.get('scene', my_scene_token)\n",
    "first_sample_token = my_scene['first_sample_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of sample tokens (of whole scene)\n",
    "current_sample_token = first_sample_token\n",
    "sample_tokens = []\n",
    "while current_sample_token != '':\n",
    "    sample_tokens.append(current_sample_token)\n",
    "    #next annotaton token\n",
    "    current_sample_token = nusc.get('sample', current_sample_token)['next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_clutter(sample_token):\n",
    "    '''\n",
    "    returns all points inside vehicle bounding boxes\n",
    "    and the number of vehicle bounding boxes that actually contain measurements\n",
    "    '''\n",
    "    current_sample_from_scene = nusc.get('sample', sample_token)\n",
    "\n",
    "    pointsensor_token = current_sample_from_scene['data']['LIDAR_TOP']\n",
    "    pcl_path = nusc.get('sample_data', pointsensor_token)['filename']\n",
    "    pc =LidarPointCloud.from_file(os.path.join('nuscenes',pcl_path))\n",
    "    points = pc.points\n",
    "    \n",
    "    ego_pose = nusc.get('ego_pose', nusc.get('sample_data', pointsensor_token)['ego_pose_token'])\n",
    "    \n",
    "\n",
    "    calibrated_sensor_token = nusc.get('sample_data', pointsensor_token)['calibrated_sensor_token']\n",
    "    calibrated_sensor = nusc.get('calibrated_sensor', calibrated_sensor_token)\n",
    "    pointsensor_quaternion = Quaternion(calibrated_sensor['rotation'])\n",
    "    ego_quaternion = Quaternion(ego_pose['rotation'])\n",
    "\n",
    "    sensor_rotated_points = np.dot(pointsensor_quaternion.rotation_matrix, points[:3,:])\n",
    "    rotated_points = np.dot(ego_quaternion.rotation_matrix, sensor_rotated_points)\n",
    "\n",
    "    # use broadcasting to add translation to x and y dimension\n",
    "    absolute_points = rotated_points+np.array(ego_pose['translation'][:3]).reshape(-1,1)\n",
    "    \n",
    "    all_annotation_points = np.empty([2, 0])\n",
    "    vehicle_counter = 0\n",
    "    for i in range(len(current_sample_from_scene['anns'])):\n",
    "\n",
    "        current_annotation_token = current_sample_from_scene['anns'][i]\n",
    "        current_annotation = nusc.get('sample_annotation', current_annotation_token)\n",
    "        if current_annotation['category_name'][:7] != 'vehicle':\n",
    "            continue\n",
    "\n",
    "        #print(f'progress: {i/len(current_sample_from_scene[\"anns\"])}')\n",
    "        ### rotate points by inverse bounding box rotation (translation to its reference frame unnecessary for only filtering)\n",
    "        reverse_annotation_quaternion = Quaternion(w=-1*current_annotation['rotation'][0], x=current_annotation['rotation'][1], y=current_annotation['rotation'][2], z=current_annotation['rotation'][3])\n",
    "        points_bounding_rotation = np.dot(reverse_annotation_quaternion.rotation_matrix, absolute_points)\n",
    "\n",
    "        #new stuff\n",
    "        relativeboxvectors, position = create_bounding_box(current_annotation_token)\n",
    "\n",
    "        rotated_box = relativeboxvectors+reverse_annotation_quaternion.rotate(position)\n",
    "\n",
    "        x_max = rotated_box[:,0].max()\n",
    "        x_min = rotated_box[:,0].min()\n",
    "\n",
    "        y_max = rotated_box[:,1].max()\n",
    "        y_min = rotated_box[:,1].min()\n",
    "\n",
    "        ## Remove points that are outside the bounding box\n",
    "        mask = np.ones(points_bounding_rotation.shape[1], dtype=bool)\n",
    "\n",
    "        mask = np.logical_and(mask, points_bounding_rotation[0] > x_min)\n",
    "        mask = np.logical_and(mask, points_bounding_rotation[0] < x_max)\n",
    "        mask = np.logical_and(mask, points_bounding_rotation[1] > y_min)\n",
    "        mask = np.logical_and(mask, points_bounding_rotation[1] < y_max)\n",
    "\n",
    "        points_filtered_rotated = points_bounding_rotation[:,mask]\n",
    "        if len(points_filtered_rotated[0])>0:\n",
    "            vehicle_counter += 1\n",
    "        ### rotate points back\n",
    "        annotation_quaternion = Quaternion(current_annotation['rotation'])\n",
    "        points_filtered_absolute = np.dot(annotation_quaternion.rotation_matrix, points_filtered_rotated)\n",
    "\n",
    "        all_annotation_points = np.concatenate((all_annotation_points,points_filtered_absolute[:2]),axis=1)\n",
    "    return all_annotation_points, vehicle_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)  # fig and axes created once\n",
    "mean_positions_whole_scene = []\n",
    "annotation_points_whole_scene = []\n",
    "#clustering by kmeans\n",
    "colors_whole_scene = []\n",
    "# number of vehicles to cluster(only matters for kmeans)\n",
    "vehicle_counters_whole_scene = []\n",
    "mean_positions_whole_scene_dbscan = []\n",
    "colors_dbscan = []\n",
    "# go over all samples of our scene\n",
    "\n",
    "for current_sample_token in sample_tokens:\n",
    "    # remove clutter\n",
    "    all_annotation_points, vehicle_counter = remove_clutter(current_sample_token)\n",
    "    annotation_points_whole_scene.append(all_annotation_points)\n",
    "    vehicle_counters_whole_scene.append(vehicle_counter)\n",
    "    \n",
    "    # apply clustering algorithms\n",
    "    ## k_means\n",
    "    clustered = sklearn.cluster.k_means(np.swapaxes(all_annotation_points,0,1), vehicle_counter)\n",
    "    mean_positions = clustered[0]\n",
    "    colors = clustered[1]\n",
    "    colors_whole_scene.append(colors)\n",
    "    mean_positions_whole_scene.append(mean_positions)\n",
    "    \n",
    "    ## dbscan\n",
    "    db = sklearn.cluster.DBSCAN(eps=1.5, min_samples=1).fit(np.swapaxes(all_annotation_points,0,1))\n",
    "\n",
    "    dbscan_clustered = [[] for x in range((db.labels_).max()+1)]\n",
    "\n",
    "    for i,point in enumerate(db.labels_):\n",
    "        dbscan_clustered[point].append(all_annotation_points[:,i])\n",
    "    mean_positions = []\n",
    "    for cluster in dbscan_clustered:\n",
    "        means = np.array(cluster).mean(axis=0)\n",
    "        mean_positions.append(means)\n",
    "    mean_positions_whole_scene_dbscan.append(mean_positions)\n",
    "    colors_dbscan.append(db.labels_)\n",
    "    \n",
    "    # add plot of mean positions to movie\n",
    "    ims.append(plt.scatter(np.swapaxes(mean_positions,0,1)[0],np.swapaxes(mean_positions,0,1)[1], s=2,).findobj())\n",
    "ax.axis('equal')\n",
    "ani = mpl.animation.ArtistAnimation(fig, ims, repeat=False)\n",
    "ani.save('annotations_im.gif', writer='pillow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p1, p2):\n",
    "    return np.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use coloring of \n",
    "#clustered with k-means: colors_whole_scene \n",
    "#or dbscan: colors_dbscan\n",
    "\n",
    "f, ax = plt.subplots(4,sharex=True)\n",
    "#ax.axis('equal')\n",
    "i = 0\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([380, 430])\n",
    "axes.set_ylim([1100,1250])\n",
    "ax[i].scatter(annotation_points_whole_scene[i][0], annotation_points_whole_scene[i][1], c=colors_dbscan[i], s=3)\n",
    "vehicle_counters_whole_scene[i]\n",
    "\n",
    "i = 1\n",
    "ax[i].scatter(annotation_points_whole_scene[i][0], annotation_points_whole_scene[i][1], c=colors_dbscan[i], s=3)\n",
    "vehicle_counters_whole_scene[i]\n",
    "\n",
    "i = 2\n",
    "ax[i].scatter(annotation_points_whole_scene[i][0], annotation_points_whole_scene[i][1], c=colors_dbscan[i], s=3)\n",
    "vehicle_counters_whole_scene[i]\n",
    "\n",
    "i = 3\n",
    "ax[i].scatter(annotation_points_whole_scene[i][0], annotation_points_whole_scene[i][1], c=colors_dbscan[i], s=3)\n",
    "vehicle_counters_whole_scene[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed_positions = mean_positions_whole_scene\n",
    "preprocessed_positions = np.array(mean_positions_whole_scene_dbscan)\n",
    "n = 0\n",
    "\n",
    "\n",
    "# create cost matrix based on distances of cluster means\n",
    "cost_matrix = np.zeros((len(preprocessed_positions[n]),len(preprocessed_positions[n+1])))\n",
    "for i in range(len(preprocessed_positions[n])):\n",
    "    for j in range(len(preprocessed_positions[n+1])):\n",
    "        cost_matrix[i,j] = distance(preprocessed_positions[n][i],preprocessed_positions[n+1][j])\n",
    "\n",
    "# algorithm to match 2 sets of points\n",
    "row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "for i in range(len(row_ind)):\n",
    "    x = [preprocessed_positions[n][row_ind[i]][0],preprocessed_positions[n+1][col_ind[i]][0]]\n",
    "    y = [preprocessed_positions[n][row_ind[i]][1],preprocessed_positions[n+1][col_ind[i]][1]]\n",
    "    print(x)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([380, 430])\n",
    "    axes.set_ylim([1100,1250])\n",
    "    plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply kalmann filter to all clusters\n",
    "## get initial velocity\n",
    "\n",
    "## get initial positions\n",
    "\n",
    "## make predictions\n",
    "\n",
    "## compare predictions to cluster means of next time step\n",
    "\n",
    "## throw out everything with velocity > 100km/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "x = np.swapaxes(mean_positions_whole_scene_dbscan[i],0,1)[0]\n",
    "y = np.swapaxes(mean_positions_whole_scene_dbscan[i],0,1)[1]\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([380, 430])\n",
    "axes.set_ylim([1100,1250])\n",
    "plt.scatter(x,y, c='red', s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "x = np.swapaxes(mean_positions_whole_scene[i],0,1)[0]\n",
    "y = np.swapaxes(mean_positions_whole_scene[i],0,1)[1]\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([380, 430])\n",
    "axes.set_ylim([1100,1250])\n",
    "plt.scatter(x,y, c='red', s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(annotation_points_whole_scene[i][0], annotation_points_whole_scene[i][1], s=0.1,c=db.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means with clutter\n",
    "a = sklearn.cluster.k_means(np.swapaxes(pc.points[:2],0,1), len(first_sample_from_scene['anns']))\n",
    "\n",
    "plt.scatter(pc.points[0], pc.points[1], s=0.1,c=a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
